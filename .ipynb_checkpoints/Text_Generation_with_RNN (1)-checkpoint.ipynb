{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e42e21",
   "metadata": {
    "id": "99e42e21"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782d69d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "782d69d5",
    "outputId": "0b98b60e-6373-4a22-9ac9-e5399611ee46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63327335",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63327335",
    "outputId": "46759743-cf90-432a-b488-0c398a5731ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding = 'utf-8')\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c56dbc",
   "metadata": {
    "id": "65c56dbc"
   },
   "source": [
    "## First 250 Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0b83aed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0b83aed",
    "outputId": "2dfa20e2-1fb0-4fa4-af13-1039d4ddf6f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b33f93a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b33f93a",
    "outputId": "a7a5bd3b-e4d5-481a-ef66-dd7446ec80bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "vocab[13:39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a605606",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a605606",
    "outputId": "a112738e-3c04-41a0-a5bd-0e1951cf0d5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Unique Characters\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(vocab)} Unique Characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b67b8f70",
   "metadata": {
    "id": "b67b8f70"
   },
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary = list(vocab), mask_token = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2afe284",
   "metadata": {
    "id": "f2afe284"
   },
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert = True, mask_token = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e7878d2",
   "metadata": {
    "id": "0e7878d2"
   },
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5cf57e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5cf57e1",
    "outputId": "5eaa4d8e-0213-43ab-d21c-95710648e012"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "795c47af",
   "metadata": {
    "id": "795c47af"
   },
   "outputs": [],
   "source": [
    "ids_Dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b43ff234",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b43ff234",
    "outputId": "02b1f9d3-0b12-49d9-b0c6-ab3b9b7c1698"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n",
      "z\n",
      "e\n",
      "n\n",
      ":\n",
      "\n",
      "\n",
      "B\n",
      "e\n",
      "f\n",
      "o\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_Dataset.take(20):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79ffaa26",
   "metadata": {
    "id": "79ffaa26"
   },
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d85f9ae",
   "metadata": {
    "id": "2d85f9ae"
   },
   "outputs": [],
   "source": [
    "sequences = ids_Dataset.batch(seq_length + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce8f0ed2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce8f0ed2",
    "outputId": "db286cac-c1b1-4f10-c195-075592c5918a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for sequence in sequences.take(1):\n",
    "    print(chars_from_ids(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2da8dd21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2da8dd21",
    "outputId": "46f60e6b-366d-4fec-8989-d31b5c015fb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n",
      "\n",
      "b'zens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but th'\n",
      "\n",
      "b'e superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we a'\n",
      "\n",
      "b're too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particula'\n",
      "\n",
      "b'rise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we bec'\n",
      "\n",
      "b'ome rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nSecond Cit'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sequence in sequences.take(10):\n",
    "    print(text_from_ids(sequence).numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "723800ed",
   "metadata": {
    "id": "723800ed"
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0e205c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0e205c0",
    "outputId": "776936f0-9540-4ac4-f88a-985a14fdcf65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Zohaib Sathi', 'ohaib Sathio')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target('Zohaib Sathio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56f6762f",
   "metadata": {
    "id": "56f6762f"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31a6e573",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31a6e573",
    "outputId": "0e5b26d6-6c31-40d0-d044-db12b482ee85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
      "b're all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for input_text, target in dataset.take(2):\n",
    "    print(text_from_ids(input_text).numpy())\n",
    "    print(text_from_ids(target).numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d0503e3",
   "metadata": {
    "id": "1d0503e3"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c3a74e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9c3a74e0",
    "outputId": "88b0cab6-5916-4d99-d91f-e1067b55216d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eca90d6c",
   "metadata": {
    "id": "eca90d6c"
   },
   "outputs": [],
   "source": [
    "# dataset = (\n",
    "#     dataset\n",
    "#     .shuffle(BUFFER_SIZE)\n",
    "#     .batch(BATCH_SIZE, drop_remainder = True)\n",
    "#     .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "# )\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ba8b7",
   "metadata": {
    "id": "d35ba8b7"
   },
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0760ff47",
   "metadata": {
    "id": "0760ff47"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afb75a85",
   "metadata": {
    "id": "afb75a85"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape = [batch_size, None])\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9528303",
   "metadata": {
    "id": "c9528303"
   },
   "outputs": [],
   "source": [
    "model = MyModel(vocab_size = vocab_size, embedding_dim = embedding_dim, rnn_units = rnn_units, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4200d66d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4200d66d",
    "outputId": "3c9c269f-a488-4612-96a3-f6667d8c59ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4e7703e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4e7703e",
    "outputId": "8da20965-9b3c-4af1-f5f2-389888cc79af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff53a6d6",
   "metadata": {
    "id": "ff53a6d6"
   },
   "outputs": [],
   "source": [
    "sample_indices = tf.random.categorical(example_batch_predictions[0], num_samples= 1)\n",
    "sample_indices = tf.squeeze(sample_indices, axis = -1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3f71668",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3f71668",
    "outputId": "7635e55c-d803-4ff1-edef-14099caf83f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 36, 24, 61,  6, 12, 65, 18, 37, 10, 62, 17, 30,  1, 20, 12, 30,\n",
       "       58,  0, 62, 46, 24, 62, 15, 14,  4,  4, 40, 30, 60, 51, 35, 32, 65,\n",
       "       45, 65, 33,  9, 18, 41, 33, 35, 39, 37, 31, 28, 56, 33, 28, 63, 30,\n",
       "       19, 27, 39,  8, 65, 38, 12, 39, 55, 35, 11, 22, 13, 26, 14, 18,  1,\n",
       "        9, 26,  1, 10, 36, 19, 36, 61, 23, 26, 37, 42, 29, 59, 25,  9, 38,\n",
       "       18,  0, 62, 49, 59,  8, 38, 31, 35, 55, 22, 29, 60,  7,  6])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "695968c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "695968c9",
    "outputId": "a6f281f8-fb40-4d80-eaa5-f734574edc5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  b' lord, ay, husband, friend!\\nI must hear from thee every day in the hour,\\nFor in a minute there are m'\n",
      "\n",
      "Output:  b\"AWKv';zEX3wDQ\\nG;Qs[UNK]wgKwBA$$aQulVSzfzT.EbTVZXROqTOxQFNZ-zY;ZpV:I?MAE\\n.M\\n3WFWvJMXcPtL.YE[UNK]wjt-YRVpIPu,'\"\n"
     ]
    }
   ],
   "source": [
    "print('Input: ', text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print('Output: ', text_from_ids(sample_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec699ad4",
   "metadata": {
    "id": "ec699ad4"
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fcf1a14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fcf1a14",
    "outputId": "0b6161da-ae79-4580-8f7b-54d933797bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Shape: (64, 100, 66)\n",
      "Loss:  tf.Tensor(4.189974, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print('Prediction Shape:', example_batch_predictions.shape)\n",
    "print('Loss: ', example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1671fde",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1671fde",
    "outputId": "4d2bdafc-1603-4a4e-fd8d-715a66a11d60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.021065"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1aa65f87",
   "metadata": {
    "id": "1aa65f87"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ac928d9",
   "metadata": {
    "id": "0ac928d9"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "021ae634",
   "metadata": {
    "id": "021ae634"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4297f022",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4297f022",
    "outputId": "c19d7445-70e1-4258-a11e-0f971a572237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 15s 60ms/step - loss: 2.7333\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 12s 58ms/step - loss: 2.0051\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 12s 58ms/step - loss: 1.7213\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 12s 60ms/step - loss: 1.5559\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 12s 61ms/step - loss: 1.4532\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 12s 61ms/step - loss: 1.3845\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 12s 60ms/step - loss: 1.3312\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 12s 57ms/step - loss: 1.2847\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 12s 60ms/step - loss: 1.2446\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 12s 61ms/step - loss: 1.2039\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 12s 61ms/step - loss: 1.1649\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 1.1237\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 1.0794\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 1.0337\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 12s 61ms/step - loss: 0.9858\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 12s 61ms/step - loss: 0.9349\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 0.8826\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 12s 61ms/step - loss: 0.8295\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 12s 62ms/step - loss: 0.7777\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 12s 60ms/step - loss: 0.7307\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "161166e8",
   "metadata": {
    "id": "161166e8"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature = 1.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "    \n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states = None):\n",
    "        inputs_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(inputs_chars).to_tensor()\n",
    "        \n",
    "        predicted_logits, states = self.model(inputs = input_ids, states = states, return_state = True)\n",
    "        \n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        \n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "        \n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "        \n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "        \n",
    "        return predicted_chars, states\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e33e681",
   "metadata": {
    "id": "9e33e681"
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bcf98cf8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcf98cf8",
    "outputId": "27b384f1-3b13-4535-c1eb-8f0d74295c96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "So much to poor, light on my duty there it,\n",
      "For I will prave the field; for is't I have not\n",
      "sent, in pain of duty, grant me in\n",
      "the sea, and let the ond unhappings for that woman,\n",
      "So I with tongue as shall redress a straw,\n",
      "And such as mine own toones may be red; I love you\n",
      "profess; the gods forbid!\n",
      "\n",
      "PETRUCHIO:\n",
      "A gentle prince: I know what would of robbers, bell;\n",
      "And that we hold arrived in the lives by him.\n",
      "The king hath been talk of woe:'d about her maid\n",
      "Nor phosing ponts at heaven fight.\n",
      "\n",
      "HASTINGS:\n",
      "Master Barnardine!\n",
      "\n",
      "Third Servingman:\n",
      "But resire thence will we desire:\n",
      "This cold may we dive me to have as brotch\n",
      "The child of this nark is dead:\n",
      "When it is but bovost, didged unto the king!\n",
      "For in a poison, good nurse, thou most beauteous tomb!\n",
      "For violents supplase, to stand it confess\n",
      "I thrown with an hundred villages in the belly.\n",
      "\n",
      "JULIET:\n",
      "Away! for that I love him well:\n",
      "Thou dost fair marching most talk of wonder;\n",
      "The cockerel's hones but scall is not hoopedre:\n",
      "There is no more mysul \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run Time:  3.893535614013672\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "    \n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun Time: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "RR8yKuUoMFFu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RR8yKuUoMFFu",
    "outputId": "d41b2b80-784e-4896-ef28-45fb84446571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 12s 61ms/step - loss: 0.6849\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 12s 62ms/step - loss: 0.6446\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 13s 62ms/step - loss: 0.6111\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 12s 58ms/step - loss: 0.5800\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 0.5552\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 12s 60ms/step - loss: 0.5315\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 12s 61ms/step - loss: 0.5137\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 0.5000\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 12s 61ms/step - loss: 0.4863\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 12s 61ms/step - loss: 0.4751\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 12s 60ms/step - loss: 0.4647\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 0.4575\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 0.4487\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 0.4463\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 12s 60ms/step - loss: 0.4411\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 12s 60ms/step - loss: 0.4368\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 12s 60ms/step - loss: 0.4321\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 12s 60ms/step - loss: 0.4307\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 0.4307\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 12s 61ms/step - loss: 0.4275\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "GC0RlqmcQ59f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GC0RlqmcQ59f",
    "outputId": "5f1baf08-1597-4b04-f4cb-3d8fb94fb352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "That love the life-was not made grows,\n",
      "And that thou oft provoked by him.\n",
      "\n",
      "LADY CAPULET:\n",
      "What! loss of such syrends from the maid to see\n",
      "Thieves of my body's vause, and stay all hard enmumber'd.\n",
      "\n",
      "BRUTUS:\n",
      "Good my lord,\n",
      "Make madness up the new of his sword reporter.\n",
      "\n",
      "CLEOMENES:\n",
      "You that be right! belike, nou woo and said;\n",
      "And be it kill the fow, and mock me with thee.\n",
      "\n",
      "Lord:\n",
      "'Tis a love! and the chape, when you are Werning does\n",
      "In her pretty man to's person from such life,\n",
      "Better your servants, reward on him.\n",
      "\n",
      "LADY CAPULET:\n",
      "Ay, say my sovereign speak! Help me not born.\n",
      "\n",
      "CAMILLO:\n",
      "When dost suspicion again, and so did I.\n",
      "\n",
      "ROMEO:\n",
      "Nay the King of Naples,\n",
      "That he should nothing else that time\n",
      "He cannot live but think your latter shake:\n",
      "Alas, poor Hellowing with my foe,\n",
      "With old obsed is happy neither; but they on thou, saving, but I\n",
      "could grow not him. I am half my wife.\n",
      "\n",
      "KATHARINA:\n",
      "Twenty cunning in the present affection to your house;\n",
      "And as he taxks, he longs to see your mejoce,\n",
      "And rathe \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run Time:  2.468486785888672\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "    \n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun Time: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ba5a7c7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba5a7c7b",
    "outputId": "ab9d43f2-e154-4808-c0eb-7239fabe790e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f52600bd910>, because it is not built.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'One_Step2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f155fbf0",
   "metadata": {
    "id": "f155fbf0"
   },
   "outputs": [],
   "source": [
    "one_step_reloaded2 = tf.saved_model.load('One_Step2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "99d1389b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99d1389b",
    "outputId": "7352fa99-2f7f-4166-e9bb-339892f37294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "By my heartily, throw forth my love\n",
      "Than body sits, and the time 'twixt sisters at home,\n",
      "I may not say is left be flayed\n",
      "In aught as much as. Nay, talk upon the heart.\n",
      "\n",
      "ESCALUS:\n",
      "Give me my boots, I w\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['JULIET:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(200):\n",
    "  next_char, states = one_step_reloaded2.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "    \n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4d223530",
   "metadata": {
    "id": "4d223530"
   },
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "24676a23",
   "metadata": {
    "id": "24676a23"
   },
   "outputs": [],
   "source": [
    "model = CustomTraining(vocab_size = len(ids_from_chars.get_vocabulary()),\n",
    "                      embedding_dim = embedding_dim,\n",
    "                      rnn_units = rnn_units,\n",
    "                      batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4bf88381",
   "metadata": {
    "id": "4bf88381"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "             loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "11a81b01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11a81b01",
    "outputId": "658d2e20-7b8b-41df-dd8d-e12f9cbcdba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "172/172 [==============================] - 16s 63ms/step - loss: 2.7189\n",
      "Epoch 2/3\n",
      "172/172 [==============================] - 13s 61ms/step - loss: 1.9855\n",
      "Epoch 3/3\n",
      "172/172 [==============================] - 15s 60ms/step - loss: 1.7049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f51ed400d00>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "tVM1AcENRvlA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVM1AcENRvlA",
    "outputId": "c5c9de79-5f24-453b-f0c9-f05511125411"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "172/172 [==============================] - 12s 57ms/step - loss: 1.5456\n",
      "Epoch 2/30\n",
      "172/172 [==============================] - 13s 59ms/step - loss: 1.4470\n",
      "Epoch 3/30\n",
      "172/172 [==============================] - 13s 61ms/step - loss: 1.3797\n",
      "Epoch 4/30\n",
      "172/172 [==============================] - 13s 61ms/step - loss: 1.3272\n",
      "Epoch 5/30\n",
      "172/172 [==============================] - 12s 57ms/step - loss: 1.2835\n",
      "Epoch 6/30\n",
      "172/172 [==============================] - 12s 58ms/step - loss: 1.2417\n",
      "Epoch 7/30\n",
      "172/172 [==============================] - 12s 58ms/step - loss: 1.2018\n",
      "Epoch 8/30\n",
      "172/172 [==============================] - 12s 60ms/step - loss: 1.1628\n",
      "Epoch 9/30\n",
      "172/172 [==============================] - 13s 60ms/step - loss: 1.1218\n",
      "Epoch 10/30\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 1.0791\n",
      "Epoch 11/30\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 1.0338\n",
      "Epoch 12/30\n",
      "172/172 [==============================] - 12s 60ms/step - loss: 0.9868\n",
      "Epoch 13/30\n",
      "172/172 [==============================] - 12s 60ms/step - loss: 0.9360\n",
      "Epoch 14/30\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 0.8848\n",
      "Epoch 15/30\n",
      "172/172 [==============================] - 13s 57ms/step - loss: 0.8335\n",
      "Epoch 16/30\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 0.7831\n",
      "Epoch 17/30\n",
      "172/172 [==============================] - 13s 60ms/step - loss: 0.7347\n",
      "Epoch 18/30\n",
      "172/172 [==============================] - 13s 61ms/step - loss: 0.6893\n",
      "Epoch 19/30\n",
      "172/172 [==============================] - 13s 60ms/step - loss: 0.6498\n",
      "Epoch 20/30\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 0.6151\n",
      "Epoch 21/30\n",
      "172/172 [==============================] - 11s 57ms/step - loss: 0.5844\n",
      "Epoch 22/30\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 0.5593\n",
      "Epoch 23/30\n",
      "172/172 [==============================] - 12s 61ms/step - loss: 0.5348\n",
      "Epoch 24/30\n",
      "172/172 [==============================] - 12s 60ms/step - loss: 0.5146\n",
      "Epoch 25/30\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 0.5006\n",
      "Epoch 26/30\n",
      "172/172 [==============================] - 11s 57ms/step - loss: 0.4885\n",
      "Epoch 27/30\n",
      "172/172 [==============================] - 12s 59ms/step - loss: 0.4781\n",
      "Epoch 28/30\n",
      "172/172 [==============================] - 13s 61ms/step - loss: 0.4681\n",
      "Epoch 29/30\n",
      "172/172 [==============================] - 13s 61ms/step - loss: 0.4580\n",
      "Epoch 30/30\n",
      "172/172 [==============================] - 13s 60ms/step - loss: 0.4504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f51ed40ba90>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "_ZJNA7KmT9wU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ZJNA7KmT9wU",
    "outputId": "491297eb-603d-4f42-9f7c-7a36eb452b7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "172/172 [==============================] - 12s 54ms/step - loss: 0.4443\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 11s 54ms/step - loss: 0.4398\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 11s 54ms/step - loss: 0.4356\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 11s 55ms/step - loss: 0.4374\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 11s 56ms/step - loss: 0.4323\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 13s 57ms/step - loss: 0.4268\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 12s 56ms/step - loss: 0.4238\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 11s 57ms/step - loss: 0.4190\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 12s 56ms/step - loss: 0.4214\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - 12s 56ms/step - loss: 0.4242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f51ed40bb50>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d6fcd36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d6fcd36",
    "outputId": "59869ac2-3a0c-4029-cd18-aaa5268b8edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Then must not on my tent them as to Cats,\n",
      "And that a widow in his face,\n",
      "Eneach it breathes, from this rightness wrongs\n",
      "Apparent him that the queen's death,\n",
      "I will some mortal to her brother to her love.\n",
      "\n",
      "HASTINGS:\n",
      "'Tis a vive, scarcely Just; and that's enough to bid me fan\n",
      "An ever been was plinted amint.\n",
      "\n",
      "LUCIO:\n",
      "\n",
      "ISABELLA:\n",
      "Could he be perfect.\n",
      "\n",
      "MENENIUS:\n",
      "Sir, sir,--\n",
      "\n",
      "SICINIUS:\n",
      "Peace, Morition, fie! 'tis like your love\n",
      "Unbected: but, as I live me, in groans,\n",
      "Your voices black stage, when you sha\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(500):\n",
    "  next_char, states = one_step_reloaded2.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "    \n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "09f95be5",
   "metadata": {
    "id": "09f95be5"
   },
   "outputs": [],
   "source": [
    "model_two = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "NOtQ4s_nV69n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOtQ4s_nV69n",
    "outputId": "3f706ebf-c78d-4591-c763-43841081eb42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "The battle calls were blows. Come, come, young Die\n",
      "His place against this right: ha! thou art most celsuitor,\n",
      "Is must be encounter sannous backs; and that we crown,\n",
      "And I will back with the manner that the very good\n",
      "French suppition.\n",
      "\n",
      "PAULINA:\n",
      "Ingendous alive to me:\n",
      "This music be many friar, and Master Stays\n",
      "To comb what I bid thee seeing in his awely;\n",
      "Call Pemer\n",
      "Seeming to all infirmation.\n",
      "I have not show'd them told us when you waked like done.\n",
      "God shall the barbe of sure men that stumbleds t\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(500):\n",
    "  next_char, states = model_two.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "    \n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9055bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$AV_ASW', '$Recycle.Bin', '$WinREAgent', 'cpp', 'Documents and Settings', 'DumpStack.log.tmp', 'Graphics Design Work', 'hiberfil.sys', 'Intel', 'Local Disk (D)', 'main.c', 'msdia80.dll', 'MSOCache', 'MUET Studies', 'My Mobile Data (Do Not Open)', 'OneDriveTemp', 'oraclexe', 'pagefile.sys', 'PerfLogs', 'Program Files', 'Program Files (x86)', 'ProgramData', 'Recovery', 'src', 'swapfile.sys', 'SWSetup', 'System Volume Information', 'Users', 'Visual Studio Codes', 'wallpapers', 'Windows', 'xampp']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb93a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
