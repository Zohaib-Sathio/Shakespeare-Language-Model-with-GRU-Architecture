{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e42e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782d69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63327335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding = 'utf-8')\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c56dbc",
   "metadata": {},
   "source": [
    "## First 250 Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0b83aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b33f93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "vocab[13:39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a605606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Unique Characters\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(vocab)} Unique Characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b67b8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary = list(vocab), mask_token = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2afe284",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert = True, mask_token = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e7878d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5cf57e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "795c47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_Dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b43ff234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n",
      "z\n",
      "e\n",
      "n\n",
      ":\n",
      "\n",
      "\n",
      "B\n",
      "e\n",
      "f\n",
      "o\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_Dataset.take(20):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79ffaa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d85f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = ids_Dataset.batch(seq_length + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce8f0ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for sequence in sequences.take(1):\n",
    "    print(chars_from_ids(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2da8dd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n",
      "\n",
      "b'zens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but th'\n",
      "\n",
      "b'e superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we a'\n",
      "\n",
      "b're too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particula'\n",
      "\n",
      "b'rise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we bec'\n",
      "\n",
      "b'ome rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\nSecond Cit'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sequence in sequences.take(10):\n",
    "    print(text_from_ids(sequence).numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "723800ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0e205c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Zohaib Sathi', 'ohaib Sathio')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target('Zohaib Sathio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56f6762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31a6e573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
      "b're all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for input_text, target in dataset.take(2):\n",
    "    print(text_from_ids(input_text).numpy())\n",
    "    print(text_from_ids(target).numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d0503e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c3a74e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eca90d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = (\n",
    "#     dataset\n",
    "#     .shuffle(BUFFER_SIZE)\n",
    "#     .batch(BATCH_SIZE, drop_remainder = True)\n",
    "#     .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "# )\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ba8b7",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0760ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afb75a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape = [batch_size, None])\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9528303",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(vocab_size = vocab_size, embedding_dim = embedding_dim, rnn_units = rnn_units, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4200d66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4e7703e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff53a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices = tf.random.categorical(example_batch_predictions[0], num_samples= 1)\n",
    "sample_indices = tf.squeeze(sample_indices, axis = -1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3f71668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 58, 53, 65, 48, 62, 63, 56,  7, 10, 63, 45, 51, 50, 51, 30, 43,\n",
       "       42, 49, 53, 39, 55, 40, 11, 19,  1, 41, 60, 15,  9,  6, 22, 15,  6,\n",
       "       14, 20, 24, 52,  2, 60,  6, 21,  4, 19, 10, 61, 26,  5,  8, 20, 34,\n",
       "       43, 55,  8, 12,  4, 13, 39, 32, 29, 64, 26, 19,  6, 24, 43,  9, 25,\n",
       "       40, 58, 46, 12, 55, 23, 59, 59, 13, 27, 61, 19, 64, 36, 51, 27, 28,\n",
       "       49, 56, 64, 37,  1, 29, 58, 22, 31,  1, 23, 31, 63,  9, 15],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "695968c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  b'e wherein he\\nwon honour than in the embracements of his bed where\\nhe would show most love. When yet '\n",
      "\n",
      "Output:  b\"Fsnziwxq,3xflklQdcjnZpa:F\\nbuB.'IB'AGKm u'H$F3vM&-GUdp-;$?ZSPyMF'Kd.Lasg;pJtt?NvFyWlNOjqyX\\nPsIR\\nJRx.B\"\n"
     ]
    }
   ],
   "source": [
    "print('Input: ', text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print('Output: ', text_from_ids(sample_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec699ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fcf1a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Shape: (64, 100, 66)\n",
      "Loss:  tf.Tensor(4.190027, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print('Prediction Shape:', example_batch_predictions.shape)\n",
    "print('Loss: ', example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1671fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.02459"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1aa65f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ac928d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "021ae634",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4297f022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 926s 5s/step - loss: 2.7323\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 682s 4s/step - loss: 1.9973\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 723s 4s/step - loss: 1.7254\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 722s 4s/step - loss: 1.5629\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 560s 3s/step - loss: 1.4612\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 551s 3s/step - loss: 1.3922\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 530s 3s/step - loss: 1.3390\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 518s 3s/step - loss: 1.2941\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 526s 3s/step - loss: 1.2534\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 514s 3s/step - loss: 1.2147\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 506s 3s/step - loss: 1.1756\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 509s 3s/step - loss: 1.1360\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 515s 3s/step - loss: 1.0946\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 505s 3s/step - loss: 1.0494\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 609s 4s/step - loss: 1.0031\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 668s 4s/step - loss: 0.9527\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 660s 4s/step - loss: 0.9015\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 665s 4s/step - loss: 0.8483\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 671s 4s/step - loss: 0.7973\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 672s 4s/step - loss: 0.7474\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "161166e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature = 1.0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "    \n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states = None):\n",
    "        inputs_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(inputs_chars).to_tensor()\n",
    "        \n",
    "        predicted_logits, states = self.model(inputs = input_ids, states = states, return_state = True)\n",
    "        \n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        \n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "        \n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "        \n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "        \n",
    "        return predicted_chars, states\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e33e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bcf98cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "I thank thee, goods, my monument long,\n",
      "I'll not, my courtsish brothery from us,\n",
      "When you should do, at the deep name blood likewise:\n",
      "'Tis felthil that hath more can I thy eyes, and her even\n",
      "Upon thy parponal glasses.\n",
      "What of this is blushes in Bohemia?\n",
      "Speak again I make all greens out.\n",
      "\n",
      "BAPTISTA:\n",
      "Why, then thou know'st it were,\n",
      "That we may just an oath by call'd men\n",
      "Both you and yours incorpsers' old fled:\n",
      "And, lords, because I was in Swife, long partners,\n",
      "Let him be call'd with reins and supple give\n",
      "Our watche of tears and makes them to see his\n",
      "shearing hath an labour nor oath Baptists deceived;\n",
      "Our soldiers, thoughts, doubt, sorrow bring him but help\n",
      "In that he starts invoteding to another,\n",
      "I, as one shepherd, buckle, 'twixt such presence.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Why, silver-for manners of the king?\n",
      "\n",
      "GLOUCESTER:\n",
      "By leave, and my promise for your great-enecle, show\n",
      "doth mantle thine to beat, worthy man,\n",
      "Heavinest false service entague.\n",
      "Hear me your sister, heaven cannot blush how much\n",
      "c \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run Time:  4.569361209869385\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "    \n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun Time: ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba5a7c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x000002015BC7F9D0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: One_Step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: One_Step\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'One_Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f155fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_reloaded = tf.saved_model.load('One_Step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "99d1389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET:\n",
      "I'll go along with thee:\n",
      "I would sooner prove liberty.\n",
      "\n",
      "KING RICHARD II:\n",
      "Ay, ay, an, so 'twretchs; my happy days!\n",
      "What servilty holds up? Do not marry why good Cominius?\n",
      "\n",
      "BIONDELLO:\n",
      "Where is thy neck\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['JULIET:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(200):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "    \n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4d223530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "24676a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomTraining(vocab_size = len(ids_from_chars.get_vocabulary()),\n",
    "                      embedding_dim = embedding_dim,\n",
    "                      rnn_units = rnn_units,\n",
    "                      batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4bf88381",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "             loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "11a81b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "172/172 [==============================] - 690s 4s/step - loss: 2.6887\n",
      "Epoch 2/3\n",
      "172/172 [==============================] - 592s 3s/step - loss: 1.9702\n",
      "Epoch 3/3\n",
      "172/172 [==============================] - 508s 3s/step - loss: 1.6987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20163b08ca0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6d6fcd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Why, hark! think you these grace\n",
      "Is three done speak thyself; but say you their\n",
      "balm and leave the sea that would have fought for Richard's?\n",
      "\n",
      "GLOUCESTER:\n",
      "Go, Durse, in the extremity of thy way:\n",
      "If Durb your blush may be achieved, were her malice?\n",
      "affection! let's salt faintable almost more\n",
      "Shall make it out anon a slaughter foe\n",
      "That wails a place believe me. O, this\n",
      "a noture, at once what is but loathed for us!\n",
      "\n",
      "PROSPERO:\n",
      "My lords, most knave all together, he should be dish'd\n",
      "hains: he stands h\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(500):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "    \n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f95be5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
